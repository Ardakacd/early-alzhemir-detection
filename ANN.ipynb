{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3acf5-3e05-4593-afcb-fede90c29505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "\n",
    "deleted_columns = ['CAEC','SMOKE','CH2O','TUE','CALC','SCC']\n",
    "\n",
    "data = data.drop(deleted_columns, axis=1)\n",
    "\n",
    "columns_to_encode = ['Gender', 'family_history_with_overweight', 'FAVC', 'MTRANS']\n",
    "\n",
    "# Select the columns to encode\n",
    "df_to_encode = data[columns_to_encode]\n",
    "\n",
    "other_columns = data.drop(columns_to_encode, axis=1)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the encoded DataFrame\n",
    "encoded_array = encoder.fit_transform(df_to_encode)\n",
    "\n",
    "# Convert the encoded array back to a DataFrame\n",
    "df_encoded_onehot = pd.DataFrame(encoded_array.toarray(), columns=encoder.get_feature_names_out(df_to_encode.columns))\n",
    "\n",
    "df_final = pd.concat([other_columns, df_encoded_onehot], axis=1)\n",
    "\n",
    "column_to_move = df_final.pop('NObeyesdad')\n",
    "\n",
    "# Reinsert the column at the end\n",
    "df_final['NObeyesdad'] = column_to_move\n",
    "\n",
    "# Custom mapping for target class (obesity level)\n",
    "feature_mapping = {\n",
    "    'Insufficient_Weight': 0,\n",
    "    'Normal_Weight': 1,\n",
    "    'Overweight_Level_I': 2,\n",
    "    'Overweight_Level_II': 3,\n",
    "    'Obesity_Type_I': 4,\n",
    "    'Obesity_Type_II': 5,\n",
    "    'Obesity_Type_III': 6,\n",
    "}\n",
    "\n",
    "df_final['NObeyesdad'] = df_final['NObeyesdad'].map(feature_mapping)\n",
    "\n",
    "X = df_final.drop('NObeyesdad', axis=1)     # Features\n",
    "y = df_final['NObeyesdad']                  # Target variable\n",
    "\n",
    "# split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=462)\n",
    "\n",
    "# min-max normalization\n",
    "min_vals = X_train.min()\n",
    "max_vals = X_train.max()\n",
    "\n",
    "column_to_be_normalized = ['Age','Height','Weight','FCVC','NCP','FAF']\n",
    "\n",
    "for col in X_train:\n",
    "    X_test[col] = (X_test[col] - min_vals[col]) / (max_vals[col] - min_vals[col])\n",
    "    X_train[col] = (X_train[col] - min_vals[col]) / (max_vals[col] - min_vals[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaceda59-7521-4a8c-9605-81201d4ba277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.8864\n",
      "Epoch [2/100], Loss: 1.6336\n",
      "Epoch [3/100], Loss: 1.3778\n",
      "Epoch [4/100], Loss: 1.1890\n",
      "Epoch [5/100], Loss: 1.1442\n",
      "Epoch [6/100], Loss: 0.8943\n",
      "Epoch [7/100], Loss: 0.7421\n",
      "Epoch [8/100], Loss: 0.7521\n",
      "Epoch [9/100], Loss: 0.5015\n",
      "Epoch [10/100], Loss: 0.4430\n",
      "Epoch [11/100], Loss: 0.6660\n",
      "Epoch [12/100], Loss: 0.4362\n",
      "Epoch [13/100], Loss: 0.3443\n",
      "Epoch [14/100], Loss: 0.2709\n",
      "Epoch [15/100], Loss: 0.3259\n",
      "Epoch [16/100], Loss: 0.3065\n",
      "Epoch [17/100], Loss: 0.1663\n",
      "Epoch [18/100], Loss: 0.2136\n",
      "Epoch [19/100], Loss: 0.2095\n",
      "Epoch [20/100], Loss: 0.1988\n",
      "Epoch [21/100], Loss: 0.1579\n",
      "Epoch [22/100], Loss: 0.4340\n",
      "Epoch [23/100], Loss: 0.2445\n",
      "Epoch [24/100], Loss: 0.1409\n",
      "Epoch [25/100], Loss: 0.1631\n",
      "Epoch [26/100], Loss: 0.2141\n",
      "Epoch [27/100], Loss: 0.1083\n",
      "Epoch [28/100], Loss: 0.1420\n",
      "Epoch [29/100], Loss: 0.2369\n",
      "Epoch [30/100], Loss: 0.3873\n",
      "Epoch [31/100], Loss: 0.0829\n",
      "Epoch [32/100], Loss: 0.2567\n",
      "Epoch [33/100], Loss: 0.2301\n",
      "Epoch [34/100], Loss: 0.1675\n",
      "Epoch [35/100], Loss: 0.0292\n",
      "Epoch [36/100], Loss: 0.0836\n",
      "Epoch [37/100], Loss: 0.0795\n",
      "Epoch [38/100], Loss: 0.0218\n",
      "Epoch [39/100], Loss: 0.0413\n",
      "Epoch [40/100], Loss: 0.0952\n",
      "Epoch [41/100], Loss: 0.0803\n",
      "Epoch [42/100], Loss: 0.1201\n",
      "Epoch [43/100], Loss: 0.1772\n",
      "Epoch [44/100], Loss: 0.0866\n",
      "Epoch [45/100], Loss: 0.0444\n",
      "Epoch [46/100], Loss: 0.0580\n",
      "Epoch [47/100], Loss: 0.1064\n",
      "Epoch [48/100], Loss: 0.0562\n",
      "Epoch [49/100], Loss: 0.0289\n",
      "Epoch [50/100], Loss: 0.0716\n",
      "Epoch [51/100], Loss: 0.0117\n",
      "Epoch [52/100], Loss: 0.1040\n",
      "Epoch [53/100], Loss: 0.0523\n",
      "Epoch [54/100], Loss: 0.1389\n",
      "Epoch [55/100], Loss: 0.0691\n",
      "Epoch [56/100], Loss: 0.1265\n",
      "Epoch [57/100], Loss: 0.2014\n",
      "Epoch [58/100], Loss: 0.0892\n",
      "Epoch [59/100], Loss: 0.0703\n",
      "Epoch [60/100], Loss: 0.0313\n",
      "Epoch [61/100], Loss: 0.0381\n",
      "Epoch [62/100], Loss: 0.0494\n",
      "Epoch [63/100], Loss: 0.0397\n",
      "Epoch [64/100], Loss: 0.0291\n",
      "Epoch [65/100], Loss: 0.0643\n",
      "Epoch [66/100], Loss: 0.0577\n",
      "Epoch [67/100], Loss: 0.0230\n",
      "Epoch [68/100], Loss: 0.0734\n",
      "Epoch [69/100], Loss: 0.0708\n",
      "Epoch [70/100], Loss: 0.0559\n",
      "Epoch [71/100], Loss: 0.0638\n",
      "Epoch [72/100], Loss: 0.1171\n",
      "Epoch [73/100], Loss: 0.2185\n",
      "Epoch [74/100], Loss: 0.0348\n",
      "Epoch [75/100], Loss: 0.0166\n",
      "Epoch [76/100], Loss: 0.1711\n",
      "Epoch [77/100], Loss: 0.0380\n",
      "Epoch [78/100], Loss: 0.0361\n",
      "Epoch [79/100], Loss: 0.1521\n",
      "Epoch [80/100], Loss: 0.0137\n",
      "Epoch [81/100], Loss: 0.0340\n",
      "Epoch [82/100], Loss: 0.0588\n",
      "Epoch [83/100], Loss: 0.0104\n",
      "Epoch [84/100], Loss: 0.0362\n",
      "Epoch [85/100], Loss: 0.0394\n",
      "Epoch [86/100], Loss: 0.0097\n",
      "Epoch [87/100], Loss: 0.0392\n",
      "Epoch [88/100], Loss: 0.1032\n",
      "Epoch [89/100], Loss: 0.0388\n",
      "Epoch [90/100], Loss: 0.0747\n",
      "Epoch [91/100], Loss: 0.0901\n",
      "Epoch [92/100], Loss: 0.0024\n",
      "Epoch [93/100], Loss: 0.0529\n",
      "Epoch [94/100], Loss: 0.0408\n",
      "Epoch [95/100], Loss: 0.0122\n",
      "Epoch [96/100], Loss: 0.0287\n",
      "Epoch [97/100], Loss: 0.0082\n",
      "Epoch [98/100], Loss: 0.1699\n",
      "Epoch [99/100], Loss: 0.0548\n",
      "Epoch [100/100], Loss: 0.1178\n",
      "\n",
      "Test Accuracy: 0.9598\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# converting training and test values into tensors\n",
    "X_train_tensor = torch.tensor(X_train.values.astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test.values.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values)\n",
    "y_test_tensor = torch.tensor(y_test.values)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "class ArtificialNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ArtificialNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.layers[-1](x)   # No activation function applied to the output layer\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_sizes = [64,64,64]           # hidden layer size is hyperparameter\n",
    "output_size = len(y.unique())\n",
    "\n",
    "model = ArtificialNeuralNetwork(input_size, hidden_sizes, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # learning rate is hyperparameter\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model.forward(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
