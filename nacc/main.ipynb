{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings(GloVe)\n",
    "word_embeddings = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/pc029sjn6y99xm1cv21rb_b40000gn/T/ipykernel_38803/2498830801.py:18: DtypeWarning: Columns (20,22,24,26,28,41,44,46,48,51,61,63,65,67,69,71,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,134,156,165,176,179,189,217,220,222,224,226,228,230,232,234,236,238,240,242,244,246,248,250,252,254,256,258,260,262,264,266,268,270,272,382,397,399,401,419,421,423,432,445,454,494,574,605,613,638,674,690,704,707,710,715,727,738,744,746,803,804,809,810,811,812,820,831,833,835,837,843,904,959,960,961,969,970,971,972,982,1004,1007,1010,1029,1034,1204,1208,1211,1407,1409,1411,1412,1414,1421,1423,1425,1426,1433,1435,1437,1439,1440,1447,1462,1476,1490,1504,1506,1542,1558,1560,1562,1564,1566,1568,1570,1572,1574,1576,1578,1580,1582,1584,1586,1588,1590,1592,1594,1596,1598,1600,1602,1604,1606,1608,1610,1612,1662,1663,1665,1666,1669,1670,1673,1674,1677,1678,1681,1682,1756,1815,1824,1826,1828,1830,1841,1843,1845,1853,1855,1857,1859,1867,1869,1871,1873,1899) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"../../nacc/newer/investigator_ftldlbd_nacc65.csv\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Add the parent folder to the system path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from artificial_nn import ANN\n",
    "import util.preprocess as preprocess\n",
    "\n",
    "# use below line for local use\n",
    "# extracted_df = pd.read_csv(\"./nacc_processed.csv\").astype(\"float\")\n",
    "\n",
    "\n",
    "# use below lines assuming you only have raw data (not filtered one)\n",
    "\n",
    "data = pd.read_csv(\"../../nacc/newer/investigator_ftldlbd_nacc65.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract columns from raw data and adjust the visit date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['VISITDAY'] = data['VISITDAY'].astype(str).str.zfill(2)\n",
    "data['VISITMO'] = data['VISITMO'].astype(str).str.zfill(2)\n",
    "data['VISITYR'] = data['VISITYR'].astype(str)\n",
    "data['VISITDT'] = data['VISITYR'] + data['VISITMO'] + data['VISITDAY']\n",
    "\n",
    "columns_to_use = ['NACCID', 'VISITDT','NACCMOCA','CRAFTDRE','COMMUN','NACCMMSE','HOMEHOBB','JUDGMENT','LOGIMEM','CDRSUM','MEMORY', 'BOSTON', 'MINTTOTS', 'ANIMALS', 'MEMUNITS', 'TRAILB', 'NACCUDSD']\n",
    "extracted_df = data[columns_to_use]\n",
    "extracted_df = extracted_df.sort_values(by=[\"NACCID\", \"VISITDT\"], ascending=True)\n",
    "new_csv = './nacc_processed.csv'\n",
    "# write the DataFrame to a CSV file\n",
    "extracted_df.to_csv(new_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (#1)\n",
    "- Forward and Backward Filling for Missing Values (Data Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_impute = columns_to_use[2:-1]\n",
    "\n",
    "def forward_and_backward_impute(group):\n",
    "    for feature in features_to_impute:\n",
    "        if feature == \"NACCMOCA\" or feature == \"NACCMMSE\":\n",
    "            group[feature] = group[feature].replace([-4,88,99], pd.NA)\n",
    "        else:\n",
    "            # replace -4 and 99 with nan for processing\n",
    "            group[feature] = group[feature].replace([-4,99], pd.NA)\n",
    "        # forward fill nan values\n",
    "        group[feature] = group[feature].ffill()\n",
    "        # backward fill nan values\n",
    "        group[feature] = group[feature].bfill()\n",
    "    return group\n",
    "\n",
    "# Group by patient_id and apply forward and backward impute function\n",
    "extracted_df = extracted_df.groupby('NACCID').apply(forward_and_backward_impute)\n",
    "\n",
    "for feature in features_to_impute:\n",
    "    extracted_df[feature] = extracted_df[feature].fillna(-4)\n",
    "\n",
    "new_csv = './fb_imputed.csv'\n",
    "# write the DataFrame to a CSV file\n",
    "extracted_df.to_csv(new_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (#2)\n",
    "- Create binary indicators for severity values (e.g. 95,...,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>NACCID</th>\n",
       "      <th>VISITDT</th>\n",
       "      <th>NACCMOCA</th>\n",
       "      <th>CRAFTDRE</th>\n",
       "      <th>COMMUN</th>\n",
       "      <th>NACCMMSE</th>\n",
       "      <th>HOMEHOBB</th>\n",
       "      <th>JUDGMENT</th>\n",
       "      <th>LOGIMEM</th>\n",
       "      <th>CDRSUM</th>\n",
       "      <th>...</th>\n",
       "      <th>ANIMALS_98</th>\n",
       "      <th>MEMUNITS_95</th>\n",
       "      <th>MEMUNITS_96</th>\n",
       "      <th>MEMUNITS_97</th>\n",
       "      <th>MEMUNITS_98</th>\n",
       "      <th>TRAILB_995</th>\n",
       "      <th>TRAILB_996</th>\n",
       "      <th>TRAILB_997</th>\n",
       "      <th>TRAILB_998</th>\n",
       "      <th>NACCUDSD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NACCID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">NACC000011</th>\n",
       "      <th>31511</th>\n",
       "      <td>NACC000011</td>\n",
       "      <td>20060417</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31512</th>\n",
       "      <td>NACC000011</td>\n",
       "      <td>20070618</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31513</th>\n",
       "      <td>NACC000011</td>\n",
       "      <td>20080603</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31514</th>\n",
       "      <td>NACC000011</td>\n",
       "      <td>20090803</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NACC000034</th>\n",
       "      <th>183244</th>\n",
       "      <td>NACC000034</td>\n",
       "      <td>20150716</td>\n",
       "      <td>27</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NACC999872</th>\n",
       "      <th>158296</th>\n",
       "      <td>NACC999872</td>\n",
       "      <td>20230321</td>\n",
       "      <td>20</td>\n",
       "      <td>9.610241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.913696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NACC999922</th>\n",
       "      <th>57864</th>\n",
       "      <td>NACC999922</td>\n",
       "      <td>20120807</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57865</th>\n",
       "      <td>NACC999922</td>\n",
       "      <td>20130806</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NACC999954</th>\n",
       "      <th>85160</th>\n",
       "      <td>NACC999954</td>\n",
       "      <td>20060327</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NACC999969</th>\n",
       "      <th>57866</th>\n",
       "      <td>NACC999969</td>\n",
       "      <td>20091102</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188700 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       NACCID   VISITDT  NACCMOCA   CRAFTDRE  COMMUN  \\\n",
       "NACCID                                                                 \n",
       "NACC000011 31511   NACC000011  20060417        -4  -4.000000     0.0   \n",
       "           31512   NACC000011  20070618        -4  -4.000000     0.0   \n",
       "           31513   NACC000011  20080603        -4  -4.000000     0.0   \n",
       "           31514   NACC000011  20090803        -4  -4.000000     0.0   \n",
       "NACC000034 183244  NACC000034  20150716        27  16.000000     0.0   \n",
       "...                       ...       ...       ...        ...     ...   \n",
       "NACC999872 158296  NACC999872  20230321        20   9.610241     0.0   \n",
       "NACC999922 57864   NACC999922  20120807        -4  -4.000000     0.0   \n",
       "           57865   NACC999922  20130806        -4  -4.000000     0.0   \n",
       "NACC999954 85160   NACC999954  20060327        -4  -4.000000     0.5   \n",
       "NACC999969 57866   NACC999969  20091102        -4  -4.000000     0.0   \n",
       "\n",
       "                   NACCMMSE  HOMEHOBB  JUDGMENT    LOGIMEM  CDRSUM  ...  \\\n",
       "NACCID                                                              ...   \n",
       "NACC000011 31511       30.0       0.0       0.0  12.000000     0.5  ...   \n",
       "           31512       29.0       0.0       0.0  13.000000     0.5  ...   \n",
       "           31513       30.0       0.0       0.0  12.000000     0.5  ...   \n",
       "           31514       28.0       0.0       0.0  11.000000     0.5  ...   \n",
       "NACC000034 183244      -4.0       0.0       0.0  -4.000000     0.5  ...   \n",
       "...                     ...       ...       ...        ...     ...  ...   \n",
       "NACC999872 158296      29.0       0.0       0.0   9.913696     0.0  ...   \n",
       "NACC999922 57864       30.0       0.0       0.0  19.000000     0.0  ...   \n",
       "           57865       30.0       0.0       0.0  12.000000     0.0  ...   \n",
       "NACC999954 85160       23.0       0.5       1.0   4.000000     4.0  ...   \n",
       "NACC999969 57866       30.0       0.0       0.0  16.000000     0.0  ...   \n",
       "\n",
       "                   ANIMALS_98  MEMUNITS_95  MEMUNITS_96  MEMUNITS_97  \\\n",
       "NACCID                                                                 \n",
       "NACC000011 31511            0            0            0            0   \n",
       "           31512            0            0            0            0   \n",
       "           31513            0            0            0            0   \n",
       "           31514            0            0            0            0   \n",
       "NACC000034 183244           0            0            0            0   \n",
       "...                       ...          ...          ...          ...   \n",
       "NACC999872 158296           0            0            0            1   \n",
       "NACC999922 57864            0            0            0            0   \n",
       "           57865            0            0            0            0   \n",
       "NACC999954 85160            0            0            0            0   \n",
       "NACC999969 57866            0            0            0            0   \n",
       "\n",
       "                   MEMUNITS_98  TRAILB_995  TRAILB_996  TRAILB_997  \\\n",
       "NACCID                                                               \n",
       "NACC000011 31511             0           0           0           0   \n",
       "           31512             0           0           0           0   \n",
       "           31513             0           0           0           0   \n",
       "           31514             0           0           0           0   \n",
       "NACC000034 183244            0           0           0           0   \n",
       "...                        ...         ...         ...         ...   \n",
       "NACC999872 158296            0           0           0           1   \n",
       "NACC999922 57864             0           0           0           0   \n",
       "           57865             0           0           0           0   \n",
       "NACC999954 85160             0           0           0           0   \n",
       "NACC999969 57866             0           0           0           0   \n",
       "\n",
       "                   TRAILB_998  NACCUDSD  \n",
       "NACCID                                   \n",
       "NACC000011 31511            0         3  \n",
       "           31512            0         3  \n",
       "           31513            0         3  \n",
       "           31514            0         2  \n",
       "NACC000034 183244           0         3  \n",
       "...                       ...       ...  \n",
       "NACC999872 158296           0         1  \n",
       "NACC999922 57864            0         1  \n",
       "           57865            0         1  \n",
       "NACC999954 85160            0         4  \n",
       "NACC999969 57866            0         1  \n",
       "\n",
       "[188700 rows x 49 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severity_value_pairs  = {\n",
    "    \"CRAFTDRE\" : [95,96,97,98],\n",
    "    \"LOGIMEM\" : [95,96,97,98],\n",
    "    \"NACCMMSE\" : [95,96,97,98],\n",
    "    \"BOSTON\" : [95,96,97,98],\n",
    "    \"MINTTOTS\" : [95,96,97,98],\n",
    "    \"ANIMALS\" : [95,96,97,98],\n",
    "    \"MEMUNITS\" : [95,96,97,98],\n",
    "    \"TRAILB\" : [995,996,997,998],\n",
    "}\n",
    "\n",
    "include_severity_vals = list(severity_value_pairs.keys())\n",
    "\n",
    "# create a seperate column for severity cases - binary indicator\n",
    "for col in include_severity_vals:\n",
    "    for val in severity_value_pairs[col]:\n",
    "        extracted_df[f\"{col}_{val}\"] = (extracted_df[col] == val).astype(int)\n",
    "\n",
    "# replace all severity values with NaN - ensure gloablity, easier to compare\n",
    "for col, severity_values in severity_value_pairs.items():\n",
    "    extracted_df[col] = extracted_df[col].replace(severity_values, np.nan)\n",
    "\n",
    "# imputation values is a dictionary that contains mean values of columns with each label\n",
    "# keys : column names , values : list with 4 (number of labels) values\n",
    "imputation_values = {}\n",
    "\n",
    "for col in severity_value_pairs.keys():\n",
    "    means = []\n",
    "    for label in range(1,5):\n",
    "        means.append(extracted_df[extracted_df[\"NACCUDSD\"] == label][col].mean())\n",
    "    imputation_values[col] = means\n",
    "\n",
    "# replace each missing value with its imputation values\n",
    "for index, row in extracted_df.iterrows():\n",
    "    for col in severity_value_pairs.keys():\n",
    "        if np.isnan(row[col]):\n",
    "            label = row[\"NACCUDSD\"]\n",
    "            imputation_val = imputation_values[col][label - 1]\n",
    "            extracted_df.at[index, col] = imputation_val\n",
    "\n",
    "main_df = extracted_df\n",
    "\n",
    "preprocess.put_the_column_at_end(main_df, \"NACCUDSD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Analysis\n",
    "- We try to reduce the number of patients with missing value\n",
    "- Can modify number of features in this stage (e.g. remove the feature with so many missing values)\n",
    "- Removal of duplicates occured from data imputation in order not to introduce bias (e.g. identical row for same patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188700\n",
      "180285\n"
     ]
    }
   ],
   "source": [
    "# handle missing values\n",
    "missing_value_pairs  = {\n",
    "    \"NACCMOCA\" : [-4,88,99],\n",
    "    \"CRAFTDRE\" : [-4],\n",
    "    \"LOGIMEM\" : [-4],\n",
    "    \"NACCMMSE\" : [-4,88],\n",
    "    \"CDRSUM\" : [99],\n",
    "    \"BOSTON\" : [-4],\n",
    "    \"MINTTOTS\" : [-4],\n",
    "    \"ANIMALS\" : [-4],\n",
    "    \"MEMUNITS\" : [-4],\n",
    "    \"TRAILB\" : [-4],\n",
    "}\n",
    "\n",
    "# replace all missing values with NaN - ensure gloablity, easier to compare\n",
    "for col, missing_values in missing_value_pairs.items():\n",
    "    main_df[col] = main_df[col].replace(missing_values, np.nan)\n",
    "\n",
    "# imputation values is a dictionary that contains mean values of columns with each label\n",
    "# keys : column names , values : list with 4 (number of labels) values\n",
    "imputation_values = {}\n",
    "\n",
    "for col in missing_value_pairs.keys():\n",
    "    means = []\n",
    "    for label in range(1,5):\n",
    "        means.append(main_df[main_df[\"NACCUDSD\"] == label][col].mean())\n",
    "    imputation_values[col] = means\n",
    "\n",
    "# replace each missing value with its imputation values\n",
    "for index, row in main_df.iterrows():\n",
    "    for col in missing_value_pairs.keys():\n",
    "        if np.isnan(row[col]):\n",
    "            label = row[\"NACCUDSD\"]\n",
    "            imputation_val = imputation_values[col][label - 1]\n",
    "            main_df.at[index, col] = imputation_val\n",
    "\n",
    "\n",
    "# due to backward and forward filling there might be few duplicate rows for the same patient\n",
    "# in order to avoid bias we can remove the duplicate rows of the patient\n",
    "columns_to_look_for = main_df.columns.difference([\"VISITDT\"])\n",
    "print(main_df.shape[0])\n",
    "main_df = main_df.drop_duplicates(subset=columns_to_look_for, keep='first')\n",
    "print(main_df.shape[0])\n",
    "# below lines can be used to visualize new matrix - ensure everything is going okay basically\n",
    "new_csv = './visualize_main_df.csv'\n",
    "main_df.to_csv(new_csv, index=False)\n",
    "\n",
    "# Replace placeholder values with NaN and count missing values\n",
    "#for column, missing_values in missing_value_pairs.items():\n",
    "#    main_df[column] = main_df[column].replace(missing_values, pd.NA)\n",
    "#    missing_count = main_df[column].isna().sum()\n",
    "#    print(f\"{column} has {missing_count} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "- Removing class imbalance using random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({1: 86302, 4: 53384, 3: 32411, 2: 8188})\n",
      "Class distribution after random undersampling: Counter({1: 8188, 2: 8188, 3: 8188, 4: 8188})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "X, y = preprocess.sep_column(main_df, \"NACCUDSD\")\n",
    "print(f\"Original class distribution: {Counter(y)}\")\n",
    "undersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = undersample.fit_resample(X, y)\n",
    "print(f\"Class distribution after random undersampling: {Counter(y_resampled)}\")\n",
    "\n",
    "main_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "new_csv = './undersampled_df.csv'\n",
    "main_df.to_csv(new_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of word embedding and NN training\n",
    "- We replace the values of the features where word embedding is appicable to use\n",
    "- We've already defined our ANN class hence, we can directly use it here for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# shape the dataset before feeding to NN\n",
    "main_df = main_df.drop([\"NACCID\",\"VISITDT\"], axis=1)\n",
    "\n",
    "suitable_word_embedded_columns = [\"COMMUN\", \"HOMEHOBB\", \"JUDGMENT\", \"MEMORY\"]\n",
    "\n",
    "# delete the rows with 99 in it (missing value)\n",
    "main_df = main_df[~main_df[suitable_word_embedded_columns].eq(99).any(axis=1)]\n",
    "\n",
    "column_combinations = [[]]\n",
    "\n",
    "# generate all combinations of different word embeddings\n",
    "for r in range(1, len(suitable_word_embedded_columns) + 1):\n",
    "    for combination in itertools.combinations(suitable_word_embedded_columns, r):\n",
    "        column_combinations.append(list(combination))\n",
    "\n",
    "string_mapping = {\n",
    "    0.0: 'no symptom',\n",
    "    0.5: 'uncertain symptom',\n",
    "    1.0: 'mild symptom',\n",
    "    2.0: 'moderate symptom',\n",
    "    3.0: 'severe symptom',\n",
    "}\n",
    "\n",
    "for word_embedded_columns in column_combinations:\n",
    "    \n",
    "    dataframe = main_df.copy(deep = True)\n",
    "\n",
    "    for col in word_embedded_columns:\n",
    "        dataframe[col] = dataframe[col].map(string_mapping)\n",
    "\n",
    "    # Features : X , Labels : y\n",
    "    X, y = preprocess.sep_column(dataframe, \"NACCUDSD\")\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    print(y.unique())\n",
    "\n",
    "    # split data into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=462)\n",
    "\n",
    "    # adjustment for pytorch nn training \n",
    "    \"\"\"\n",
    "    The targets should be in the range [0, 3] for our use case, as they are used to index the output tensor\n",
    "    \"\"\"\n",
    "    y_train = y_train - 1\n",
    "    y_test = y_test - 1\n",
    "\n",
    "\n",
    "    X_train = preprocess.replace_with_word_embeddings(X_train, word_embeddings, word_embedded_columns)\n",
    "    X_test = preprocess.replace_with_word_embeddings(X_test, word_embeddings, word_embedded_columns)\n",
    "        \n",
    "    X_train_tensor = torch.tensor(X_train.values.astype(np.float32))\n",
    "    X_test_tensor = torch.tensor(X_test.values.astype(np.float32))\n",
    "    y_train_tensor = torch.tensor(y_train.values)\n",
    "    y_test_tensor = torch.tensor(y_test.values)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    input_size = X_train_tensor.shape[1]\n",
    "    hidden_sizes = [512,256,128,64]               # hidden layer size is hyperparameter\n",
    "    output_size = len(y.unique())\n",
    "    \n",
    "    model = ANN.ArtificialNeuralNetwork(input_size, hidden_sizes, output_size)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()                       # cross entropy value is used as loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)    # learning rate is hyperparameter\n",
    "    \n",
    "    num_epochs = 500\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss_of_epoch = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()               # clears the gradients before new back prop (new batch)\n",
    "            outputs = model.forward(inputs)     # feed model with forward prop (get predictions - outputs)\n",
    "            loss = criterion(outputs, labels)   # calculate loss value of predictions\n",
    "            loss.backward()                     # perform back prop to compute gradient w.r.t model params\n",
    "            optimizer.step()                    # update the model params (weights) according to LR and gradient\n",
    "\n",
    "            loss_of_epoch += loss.item()\n",
    "\n",
    "        loss_list.append(loss_of_epoch / len(train_loader))\n",
    "    \n",
    "        #print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    plt.plot(range(1, num_epochs + 1), loss_list, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss Curve for replacing {\" , \".join(word_embedded_columns)} attribute with word embedding vector')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # evaluation of model in test set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model.forward(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_tensor.cpu(), predicted.cpu(), digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
