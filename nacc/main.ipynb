{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings(GloVe)\n",
    "word_embeddings = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NACCMOCA   CRAFTDRE  COMMUN   NACCMMSE  HOMEHOBB  JUDGMENT  \\\n",
      "1       23.000000  14.000000     0.0  28.905319       0.0       0.0   \n",
      "2        5.000000   0.000000     1.0  19.176808       3.0       1.0   \n",
      "3       29.000000  13.000000     0.0  28.905319       0.0       0.0   \n",
      "5       14.000000   9.000000     0.0  19.176808       0.0       0.5   \n",
      "6       17.000000   6.000000     0.5  19.176808       0.5       0.5   \n",
      "...           ...        ...     ...        ...       ...       ...   \n",
      "188692  28.000000  16.000000     0.0  28.905319       0.0       0.0   \n",
      "188693  26.482423  20.000000     0.0  28.905319       0.0       0.0   \n",
      "188696  14.933572   3.833011     0.5  26.000000       0.5       0.5   \n",
      "188698  14.933572   3.833011     0.0  28.000000       0.5       0.5   \n",
      "188699  14.933572   3.833011     0.5  27.000000       0.5       0.5   \n",
      "\n",
      "          LOGIMEM  CDRSUM  MEMORY     BOSTON  ...  LOGIMEM_97  LOGIMEM_98  \\\n",
      "1       14.468402     0.0     0.0  27.360985  ...           0           0   \n",
      "2        4.250537     9.0     2.0  18.695350  ...           0           0   \n",
      "3       14.468402     0.0     0.0  27.360985  ...           0           0   \n",
      "5        4.250537     1.0     0.5  18.695350  ...           0           0   \n",
      "6        4.250537     3.0     1.0  18.695350  ...           0           0   \n",
      "...           ...     ...     ...        ...  ...         ...         ...   \n",
      "188692  14.468402     0.0     0.0  27.360985  ...           0           0   \n",
      "188693  14.468402     0.0     0.0  27.360985  ...           0           0   \n",
      "188696  14.000000     3.0     1.0  22.000000  ...           0           0   \n",
      "188698  10.000000     1.5     0.5  27.000000  ...           0           0   \n",
      "188699   7.000000     2.5     0.5  27.000000  ...           0           0   \n",
      "\n",
      "        NACCMMSE_95  NACCMMSE_96  NACCMMSE_97  NACCMMSE_98  BOSTON_95  \\\n",
      "1                 0            0            0            0          0   \n",
      "2                 0            0            0            0          0   \n",
      "3                 0            0            0            0          0   \n",
      "5                 0            0            0            0          0   \n",
      "6                 0            0            0            0          0   \n",
      "...             ...          ...          ...          ...        ...   \n",
      "188692            0            0            0            0          0   \n",
      "188693            0            0            0            0          0   \n",
      "188696            0            0            0            0          0   \n",
      "188698            0            0            0            0          0   \n",
      "188699            0            0            0            0          0   \n",
      "\n",
      "        BOSTON_96  BOSTON_97  BOSTON_98  \n",
      "1               0          0          0  \n",
      "2               0          0          0  \n",
      "3               0          0          0  \n",
      "5               0          0          0  \n",
      "6               0          0          0  \n",
      "...           ...        ...        ...  \n",
      "188692          0          0          0  \n",
      "188693          0          0          0  \n",
      "188696          0          0          0  \n",
      "188698          0          0          0  \n",
      "188699          0          0          0  \n",
      "\n",
      "[147397 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Add the parent folder to the system path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from artificial_nn import ANN\n",
    "import util.preprocess as preprocess\n",
    "\n",
    "# use below line for local use\n",
    "extracted_df = pd.read_csv(\"./nacc_processed.csv\").astype(\"float\")\n",
    "\n",
    "\n",
    "# use below lines assuming you only have raw data (not filtered one)\n",
    "\n",
    "#data = pd.read_csv(\"../../nacc/newer/investigator_ftldlbd_nacc65.csv\")\n",
    "#columns_to_use = ['NACCMOCA','CRAFTDRE','COMMUN','NACCMMSE','HOMEHOBB','JUDGMENT','LOGIMEM','CDRSUM','MEMORY', 'BOSTON', 'NACCUDSD']\n",
    "#extracted_df = data[columns_to_use]\n",
    "#print(extracted_df.head())\n",
    "#new_csv = './nacc_processed.csv'\n",
    "# Write the DataFrame to a CSV file\n",
    "#extracted_df.to_csv(new_csv, index=False)\n",
    "\n",
    "# handle missing values\n",
    "missing_value_pairs  = {\n",
    "    \"NACCMOCA\" : [-4,88,99],\n",
    "    \"CRAFTDRE\" : [-4,95,96,97,98],\n",
    "    \"LOGIMEM\" : [-4,95,96,97,98],\n",
    "    \"NACCMMSE\" : [-4,88,95,96,97,98],\n",
    "    \"CDRSUM\" : [99],\n",
    "    \"BOSTON\" : [-4,95,96,97,98]\n",
    "}\n",
    "\n",
    "include_severity_vals = [\"CRAFTDRE\", \"LOGIMEM\", \"NACCMMSE\", \"BOSTON\"]\n",
    "\n",
    "# create a seperate column for severity cases - binary indicator\n",
    "for col in include_severity_vals:\n",
    "    for val in [95,96,97,98]:\n",
    "        extracted_df[f\"{col}_{val}\"] = (extracted_df[col] == val).astype(int)\n",
    "\n",
    "# replace all missing values with NaN - ensure gloablity, easier to compare\n",
    "for col, missing_values in missing_value_pairs.items():\n",
    "    extracted_df[col] = extracted_df[col].replace(missing_values, np.nan)\n",
    "\n",
    "# imputation values is a dictionary that contains mean values of columns with each label\n",
    "# keys : column names , values : list with 4 (number of labels) values\n",
    "imputation_values = {}\n",
    "\n",
    "for col in missing_value_pairs.keys():\n",
    "    means = []\n",
    "    for label in range(1,5):\n",
    "        means.append(extracted_df[extracted_df[\"NACCUDSD\"] == label][col].mean())\n",
    "    imputation_values[col] = means\n",
    "\n",
    "# replace each missing value with its imputation values\n",
    "for index, row in extracted_df.iterrows():\n",
    "    for col in missing_value_pairs.keys():\n",
    "        if np.isnan(row[col]):\n",
    "            label = row[\"NACCUDSD\"].astype(\"int64\")\n",
    "            imputation_val = imputation_values[col][label - 1]\n",
    "            extracted_df.at[index, col] = imputation_val\n",
    "\n",
    "# use below lines to filter the dataset using label values\n",
    "# e.g. [1,4] will filter dataset such that only rows that labeled with 1 and 4 will remain\n",
    "labels_to_include = [1,4]\n",
    "main_df = preprocess.filter_by_label(extracted_df, \"NACCUDSD\", labels_to_include)\n",
    "print(main_df)\n",
    "\n",
    "\n",
    "#main_df = extracted_df\n",
    "\n",
    "# below lines can be used to visualize new matrix - ensure everything is going okay basically\n",
    "\n",
    "#new_csv = './visualize_main_df.csv'\n",
    "#extracted_df.to_csv(new_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "suitable_word_embedded_columns = [\"COMMUN\", \"HOMEHOBB\", \"JUDGMENT\", \"MEMORY\"]\n",
    "\n",
    "# delete the rows with 99 in it (missing value)\n",
    "main_df = main_df[~main_df[suitable_word_embedded_columns].eq(99).any(axis=1)]\n",
    "\n",
    "column_combinations = [[]]\n",
    "\n",
    "# generate all combinations of different word embeddings\n",
    "for r in range(1, len(suitable_word_embedded_columns) + 1):\n",
    "    for combination in itertools.combinations(suitable_word_embedded_columns, r):\n",
    "        column_combinations.append(list(combination))\n",
    "\n",
    "#print(extracted_df[\"COMMUN\"].unique())\n",
    "#print(extracted_df[\"JUDGMENT\"].unique())\n",
    "#print(extracted_df[\"HOMEHOBB\"].unique())\n",
    "#print(extracted_df[\"MEMORY\"].unique())\n",
    "\n",
    "string_mapping = {\n",
    "    0.0: 'no symptom',\n",
    "    0.5: 'uncertain symptom',\n",
    "    1.0: 'mild symptom',\n",
    "    2.0: 'moderate symptom',\n",
    "    3.0: 'severe symptom',\n",
    "    # 99 : 'default'\n",
    "}\n",
    "\n",
    "for word_embedded_columns in column_combinations:\n",
    "    \n",
    "    dataframe = main_df.copy(deep = True)\n",
    "\n",
    "    for col in word_embedded_columns:\n",
    "        dataframe[col] = dataframe[col].map(string_mapping)\n",
    "\n",
    "    # Features : X , Labels : y\n",
    "    X, y = preprocess.sep_column(dataframe, \"NACCUDSD\")\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    # split data into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=462)\n",
    "\n",
    "    # adjustment for pytorch nn training \n",
    "    \"\"\"\n",
    "    The targets should be in the range [0, 3] for our use case, as they are used to index the output tensor\n",
    "    \"\"\"   \n",
    "    # match 1 to 0 and 4 to 1\n",
    "    y_train = (y_train == 4).astype(int)\n",
    "    y_test = (y_test == 4).astype(int)\n",
    "\n",
    "\n",
    "    #y_train = y_train - 1\n",
    "    #y_test = y_test - 1\n",
    "\n",
    "    X_train = preprocess.replace_with_word_embeddings(X_train, word_embeddings, word_embedded_columns)\n",
    "    X_test = preprocess.replace_with_word_embeddings(X_test, word_embeddings, word_embedded_columns)\n",
    "    \n",
    "    #print(X_train)\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train.values.astype(np.float32))\n",
    "    X_test_tensor = torch.tensor(X_test.values.astype(np.float32))\n",
    "    y_train_tensor = torch.tensor(y_train.values)\n",
    "    y_test_tensor = torch.tensor(y_test.values)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    input_size = X_train_tensor.shape[1]\n",
    "    hidden_sizes = [512,512,256,128,64]               # hidden layer size is hyperparameter\n",
    "    output_size = len(y.unique())\n",
    "    \n",
    "    model = ANN.ArtificialNeuralNetwork(input_size, hidden_sizes, output_size)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()                       # cross entropy value is used as loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)    # learning rate is hyperparameter\n",
    "    \n",
    "    num_epochs = 100\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss_of_epoch = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()               # clears the gradients before new back prop (new batch)\n",
    "            outputs = model.forward(inputs)     # feed model with forward prop (get predictions - outputs)\n",
    "            loss = criterion(outputs, labels)   # calculate loss value of predictions\n",
    "            loss.backward()                     # perform back prop to compute gradient w.r.t model params\n",
    "            optimizer.step()                    # update the model params (weights) according to LR and gradient\n",
    "\n",
    "            loss_of_epoch += loss.item()\n",
    "\n",
    "        loss_list.append(loss_of_epoch / len(train_loader))\n",
    "    \n",
    "        #print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    plt.plot(range(1, num_epochs + 1), loss_list, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss Curve for replacing {\" , \".join(word_embedded_columns)} attribute with word embedding vector')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # evaluation of model in test set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model.forward(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_tensor.cpu(), predicted.cpu(), digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
