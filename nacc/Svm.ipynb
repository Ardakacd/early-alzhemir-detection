{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f6798-5fb6-431d-97be-67b7772f34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings(GloVe)\n",
    "word_embeddings = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f376801-d083-45de-b329-0704d6c8e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Add the parent folder to the system path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from artificial_nn import ANN\n",
    "import util.preprocess as preprocess\n",
    "\n",
    "# use below line for local use\n",
    "# extracted_df = pd.read_csv(\"./nacc_processed.csv\").astype(\"float\")\n",
    "\n",
    "\n",
    "# use below lines assuming you only have raw data (not filtered one)\n",
    "\n",
    "data = pd.read_csv(\"./investigator_ftldlbd_nacc65.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df350b-764f-4db4-a062-d4cd8c78bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['VISITDAY'] = data['VISITDAY'].astype(str).str.zfill(2)\n",
    "data['VISITMO'] = data['VISITMO'].astype(str).str.zfill(2)\n",
    "data['VISITYR'] = data['VISITYR'].astype(str)\n",
    "data['VISITDT'] = data['VISITYR'] + data['VISITMO'] + data['VISITDAY']\n",
    "\n",
    "columns_to_use = ['NACCID', 'VISITDT','NACCMOCA','CRAFTDRE','COMMUN','NACCMMSE','HOMEHOBB','JUDGMENT','LOGIMEM','CDRSUM','MEMORY', 'BOSTON', 'MINTTOTS', 'ANIMALS', 'MEMUNITS', 'TRAILB', 'NACCUDSD']\n",
    "extracted_df = data[columns_to_use]\n",
    "extracted_df = extracted_df.sort_values(by=[\"NACCID\", \"VISITDT\"], ascending=True)\n",
    "new_csv = './nacc_processed.csv'\n",
    "# write the DataFrame to a CSV file\n",
    "extracted_df.to_csv(new_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f90d6-8b92-4eb1-9ac3-4da13fb7e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_impute = columns_to_use[2:-1]\n",
    "\n",
    "def forward_and_backward_impute(group):\n",
    "    for feature in features_to_impute:\n",
    "        if feature == \"NACCMOCA\" or feature == \"NACCMMSE\":\n",
    "            group[feature] = group[feature].replace([-4,88,99], pd.NA)\n",
    "        else:\n",
    "            # replace -4 and 99 with nan for processing\n",
    "            group[feature] = group[feature].replace([-4,99], pd.NA)\n",
    "        # forward fill nan values\n",
    "        group[feature] = group[feature].ffill()\n",
    "        # backward fill nan values\n",
    "        group[feature] = group[feature].bfill()\n",
    "    return group\n",
    "\n",
    "# Group by patient_id and apply forward and backward impute function\n",
    "extracted_df = extracted_df.groupby('NACCID').apply(forward_and_backward_impute)\n",
    "\n",
    "for feature in features_to_impute:\n",
    "    extracted_df[feature] = extracted_df[feature].fillna(-4)\n",
    "\n",
    "new_csv = './fb_imputed.csv'\n",
    "# write the DataFrame to a CSV file\n",
    "extracted_df.to_csv(new_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a7c88-7aa1-4964-a78c-4d1669c22764",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_value_pairs  = {\n",
    "    \"CRAFTDRE\" : [95,96,97,98],\n",
    "    \"LOGIMEM\" : [95,96,97,98],\n",
    "    \"NACCMMSE\" : [95,96,97,98],\n",
    "    \"BOSTON\" : [95,96,97,98],\n",
    "    \"MINTTOTS\" : [95,96,97,98],\n",
    "    \"ANIMALS\" : [95,96,97,98],\n",
    "    \"MEMUNITS\" : [95,96,97,98],\n",
    "    \"TRAILB\" : [995,996,997,998],\n",
    "}\n",
    "\n",
    "include_severity_vals = list(severity_value_pairs.keys())\n",
    "\n",
    "# create a seperate column for severity cases - binary indicator\n",
    "for col in include_severity_vals:\n",
    "    for val in severity_value_pairs[col]:\n",
    "        extracted_df[f\"{col}_{val}\"] = (extracted_df[col] == val).astype(int)\n",
    "\n",
    "# replace all severity values with NaN - ensure gloablity, easier to compare\n",
    "for col, severity_values in severity_value_pairs.items():\n",
    "    extracted_df[col] = extracted_df[col].replace(severity_values, np.nan)\n",
    "\n",
    "# imputation values is a dictionary that contains mean values of columns with each label\n",
    "# keys : column names , values : list with 4 (number of labels) values\n",
    "imputation_values = {}\n",
    "\n",
    "for col in severity_value_pairs.keys():\n",
    "    means = []\n",
    "    for label in range(1,5):\n",
    "        means.append(extracted_df[extracted_df[\"NACCUDSD\"] == label][col].mean())\n",
    "    imputation_values[col] = means\n",
    "\n",
    "# replace each missing value with its imputation values\n",
    "for index, row in extracted_df.iterrows():\n",
    "    for col in severity_value_pairs.keys():\n",
    "        if np.isnan(row[col]):\n",
    "            label = row[\"NACCUDSD\"]\n",
    "            imputation_val = imputation_values[col][label - 1]\n",
    "            extracted_df.at[index, col] = imputation_val\n",
    "\n",
    "main_df = extracted_df\n",
    "\n",
    "preprocess.put_the_column_at_end(main_df, \"NACCUDSD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace0ba5-7102-42b7-856b-13fc1711ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing values\n",
    "missing_value_pairs  = {\n",
    "    \"NACCMOCA\" : [-4,88,99],\n",
    "    \"CRAFTDRE\" : [-4],\n",
    "    \"LOGIMEM\" : [-4],\n",
    "    \"NACCMMSE\" : [-4,88],\n",
    "    \"CDRSUM\" : [99],\n",
    "    \"BOSTON\" : [-4],\n",
    "    \"MINTTOTS\" : [-4],\n",
    "    \"ANIMALS\" : [-4],\n",
    "    \"MEMUNITS\" : [-4],\n",
    "    \"TRAILB\" : [-4],\n",
    "}\n",
    "\n",
    "# replace all missing values with NaN - ensure gloablity, easier to compare\n",
    "for col, missing_values in missing_value_pairs.items():\n",
    "    main_df[col] = main_df[col].replace(missing_values, np.nan)\n",
    "\n",
    "# imputation values is a dictionary that contains mean values of columns with each label\n",
    "# keys : column names , values : list with 4 (number of labels) values\n",
    "imputation_values = {}\n",
    "\n",
    "for col in missing_value_pairs.keys():\n",
    "    means = []\n",
    "    for label in range(1,5):\n",
    "        means.append(main_df[main_df[\"NACCUDSD\"] == label][col].mean())\n",
    "    imputation_values[col] = means\n",
    "\n",
    "# replace each missing value with its imputation values\n",
    "for index, row in main_df.iterrows():\n",
    "    for col in missing_value_pairs.keys():\n",
    "        if np.isnan(row[col]):\n",
    "            label = row[\"NACCUDSD\"]\n",
    "            imputation_val = imputation_values[col][label - 1]\n",
    "            main_df.at[index, col] = imputation_val\n",
    "\n",
    "\n",
    "# due to backward and forward filling there might be few duplicate rows for the same patient\n",
    "# in order to avoid bias we can remove the duplicate rows of the patient\n",
    "columns_to_look_for = main_df.columns.difference([\"VISITDT\"])\n",
    "print(main_df.shape[0])\n",
    "main_df = main_df.drop_duplicates(subset=columns_to_look_for, keep='first')\n",
    "print(main_df.shape[0])\n",
    "# below lines can be used to visualize new matrix - ensure everything is going okay basically\n",
    "new_csv = './visualize_main_df.csv'\n",
    "main_df.to_csv(new_csv, index=False)\n",
    "\n",
    "# Replace placeholder values with NaN and count missing values\n",
    "#for column, missing_values in missing_value_pairs.items():\n",
    "#    main_df[column] = main_df[column].replace(missing_values, pd.NA)\n",
    "#    missing_count = main_df[column].isna().sum()\n",
    "#    print(f\"{column} has {missing_count} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efeb06-a6bf-4725-b74b-f50acd8b7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "X, y = preprocess.sep_column(main_df, \"NACCUDSD\")\n",
    "print(f\"Original class distribution: {Counter(y)}\")\n",
    "undersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = undersample.fit_resample(X, y)\n",
    "print(f\"Class distribution after random undersampling: {Counter(y_resampled)}\")\n",
    "\n",
    "main_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "new_csv = './undersampled_df.csv'\n",
    "main_df.to_csv(new_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ae53f-9627-4d82-8785-9db989db5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from svm.svm import OneVsAllSVM\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "suitable_word_embedded_columns = [\"COMMUN\", \"HOMEHOBB\", \"JUDGMENT\", \"MEMORY\"]\n",
    "\n",
    "main_df = pd.read_csv(\"./undersampled_df.csv\")\n",
    "\n",
    "main_df = main_df[main_df['NACCUDSD'] != 2]\n",
    "main_df['NACCUDSD'] = main_df['NACCUDSD'].replace({3: 2, 4: 3})\n",
    "\n",
    "# delete the rows with 99 in it (missing value)\n",
    "main_df = main_df[~main_df[suitable_word_embedded_columns].eq(99).any(axis=1)]\n",
    "columns_to_drop = [\"NACCID\", \"VISITDT\"]\n",
    "\n",
    "# Check if the columns exist in the DataFrame\n",
    "columns_to_drop = [col for col in columns_to_drop if col in main_df.columns]\n",
    "\n",
    "# Drop the columns if they exist\n",
    "if columns_to_drop:\n",
    "    main_df = main_df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "column_combinations = [[\"COMMUN\", \"HOMEHOBB\", \"JUDGMENT\", \"MEMORY\"]]\n",
    "\n",
    "# generate all combinations of different word embeddings\n",
    "#for r in range(1, len(suitable_word_embedded_columns) + 1):\n",
    "#    for combination in itertools.combinations(suitable_word_embedded_columns, r):\n",
    "#        column_combinations.append(list(combination))\n",
    "\n",
    "string_mapping = {\n",
    "    0.0: 'no symptom',\n",
    "    0.5: 'uncertain symptom',\n",
    "    1.0: 'mild symptom',\n",
    "    2.0: 'moderate symptom',\n",
    "    3.0: 'severe symptom',\n",
    "}\n",
    "\n",
    "for word_embedded_columns in column_combinations:\n",
    "    \n",
    "    dataframe = main_df.copy(deep = True)\n",
    "\n",
    "    for col in word_embedded_columns:\n",
    "        dataframe[col] = dataframe[col].map(string_mapping)\n",
    "\n",
    "    # Features : X , Labels : y\n",
    "    X, y = preprocess.sep_column(dataframe, \"NACCUDSD\")\n",
    "    y = y.astype(int)\n",
    "\n",
    "    # split data into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=462)\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    y_train = y_train - 1\n",
    "    y_test = y_test - 1\n",
    "\n",
    "    \n",
    "    svm = OneVsAllSVM()\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm.visualize()\n",
    "    svm.report(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e2c35-6ef1-4b29-86d1-f69286e249f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
