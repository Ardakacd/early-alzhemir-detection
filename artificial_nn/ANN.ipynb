{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e71d81",
   "metadata": {},
   "source": [
    "### Preparing the Dataset\n",
    "In this block we've pre-processed the provided dataset according to our needs. \n",
    "* We've applied feature selection and normalization on dataset.\n",
    "* Dataset is splitted into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec3acf5-3e05-4593-afcb-fede90c29505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Add the parent folder to the system path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from util import preprocess\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"../obesity_impl/ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "\n",
    "deleted_columns = ['CAEC','SMOKE','CH2O','TUE','CALC','SCC']\n",
    "\n",
    "data = data.drop(deleted_columns, axis=1)\n",
    "\n",
    "columns_to_encode = ['Gender', 'family_history_with_overweight', 'FAVC', 'MTRANS']\n",
    "\n",
    "# Select the columns to encode\n",
    "df_to_encode = data[columns_to_encode]\n",
    "\n",
    "other_columns = data.drop(columns_to_encode, axis=1)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the encoded DataFrame\n",
    "encoded_array = encoder.fit_transform(df_to_encode)\n",
    "\n",
    "# Convert the encoded array back to a DataFrame\n",
    "df_encoded_onehot = pd.DataFrame(encoded_array.toarray(), columns=encoder.get_feature_names_out(df_to_encode.columns))\n",
    "\n",
    "df_final = pd.concat([other_columns, df_encoded_onehot], axis=1)\n",
    "\n",
    "preprocess.put_the_column_at_end(df_final,\"NObeyesdad\")\n",
    "\n",
    "# Custom mapping for target class (obesity level)\n",
    "feature_mapping = {\n",
    "    'Insufficient_Weight': 0,\n",
    "    'Normal_Weight': 1,\n",
    "    'Overweight_Level_I': 2,\n",
    "    'Overweight_Level_II': 3,\n",
    "    'Obesity_Type_I': 4,\n",
    "    'Obesity_Type_II': 5,\n",
    "    'Obesity_Type_III': 6,\n",
    "}\n",
    "\n",
    "df_final['NObeyesdad'] = df_final['NObeyesdad'].map(feature_mapping)\n",
    "\n",
    "X = df_final.drop('NObeyesdad', axis=1)     # Features\n",
    "y = df_final['NObeyesdad']                  # Target variable\n",
    "\n",
    "# split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=462)\n",
    "\n",
    "# min-max normalization\n",
    "min_vals = X_train.min()\n",
    "max_vals = X_train.max()\n",
    "\n",
    "column_to_be_normalized = ['Age','Height','Weight','FCVC','NCP','FAF']\n",
    "\n",
    "for col in X_train:\n",
    "    if col in column_to_be_normalized:\n",
    "        X_test[col] = (X_test[col] - min_vals[col]) / (max_vals[col] - min_vals[col])\n",
    "        X_train[col] = (X_train[col] - min_vals[col]) / (max_vals[col] - min_vals[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925fab89",
   "metadata": {},
   "source": [
    "### Definition of Neural Network Model\n",
    "We've defined our NN model and relevant learning and optimization parameters\n",
    "* Data conversion to tensors (basically to make PyTorch to make use of them)\n",
    "* Defined layer sizes and activation function being used in forward prop\n",
    "* Defined the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaceda59-7521-4a8c-9605-81201d4ba277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ANN import ArtificialNeuralNetwork\n",
    "\n",
    "# converting training and test values into tensors\n",
    "X_train_tensor = torch.tensor(X_train.values.astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test.values.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values)\n",
    "y_test_tensor = torch.tensor(y_test.values)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_sizes = [64,64,64]               # hidden layer size is hyperparameter\n",
    "output_size = len(y.unique())\n",
    "\n",
    "model = ArtificialNeuralNetwork(input_size, hidden_sizes, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()                       # cross entropy value is used as loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)    # learning rate is hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b773af0",
   "metadata": {},
   "source": [
    "### Training\n",
    "We've trained our model for number of epochs with the predefined (by us) learning rate. Number of epochs can be determined emprically while keeping mind that the convergence time and value of the model also dependent on the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49502bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.8905\n",
      "Epoch [2/100], Loss: 1.5709\n",
      "Epoch [3/100], Loss: 1.3858\n",
      "Epoch [4/100], Loss: 1.2916\n",
      "Epoch [5/100], Loss: 1.4083\n",
      "Epoch [6/100], Loss: 0.9961\n",
      "Epoch [7/100], Loss: 1.0824\n",
      "Epoch [8/100], Loss: 0.9817\n",
      "Epoch [9/100], Loss: 0.8795\n",
      "Epoch [10/100], Loss: 0.8678\n",
      "Epoch [11/100], Loss: 0.7911\n",
      "Epoch [12/100], Loss: 0.6510\n",
      "Epoch [13/100], Loss: 0.3716\n",
      "Epoch [14/100], Loss: 0.7214\n",
      "Epoch [15/100], Loss: 0.5632\n",
      "Epoch [16/100], Loss: 0.2924\n",
      "Epoch [17/100], Loss: 0.3668\n",
      "Epoch [18/100], Loss: 0.2715\n",
      "Epoch [19/100], Loss: 0.2928\n",
      "Epoch [20/100], Loss: 0.3371\n",
      "Epoch [21/100], Loss: 0.2572\n",
      "Epoch [22/100], Loss: 0.2465\n",
      "Epoch [23/100], Loss: 0.3489\n",
      "Epoch [24/100], Loss: 0.1901\n",
      "Epoch [25/100], Loss: 0.1956\n",
      "Epoch [26/100], Loss: 0.0618\n",
      "Epoch [27/100], Loss: 0.1522\n",
      "Epoch [28/100], Loss: 0.1094\n",
      "Epoch [29/100], Loss: 0.1182\n",
      "Epoch [30/100], Loss: 0.1302\n",
      "Epoch [31/100], Loss: 0.0574\n",
      "Epoch [32/100], Loss: 0.0841\n",
      "Epoch [33/100], Loss: 0.1178\n",
      "Epoch [34/100], Loss: 0.0871\n",
      "Epoch [35/100], Loss: 0.0888\n",
      "Epoch [36/100], Loss: 0.0604\n",
      "Epoch [37/100], Loss: 0.1188\n",
      "Epoch [38/100], Loss: 0.0716\n",
      "Epoch [39/100], Loss: 0.0779\n",
      "Epoch [40/100], Loss: 0.0938\n",
      "Epoch [41/100], Loss: 0.1291\n",
      "Epoch [42/100], Loss: 0.0974\n",
      "Epoch [43/100], Loss: 0.0424\n",
      "Epoch [44/100], Loss: 0.1909\n",
      "Epoch [45/100], Loss: 0.0798\n",
      "Epoch [46/100], Loss: 0.0467\n",
      "Epoch [47/100], Loss: 0.0737\n",
      "Epoch [48/100], Loss: 0.0740\n",
      "Epoch [49/100], Loss: 0.0737\n",
      "Epoch [50/100], Loss: 0.1457\n",
      "Epoch [51/100], Loss: 0.1118\n",
      "Epoch [52/100], Loss: 0.1262\n",
      "Epoch [53/100], Loss: 0.0376\n",
      "Epoch [54/100], Loss: 0.0358\n",
      "Epoch [55/100], Loss: 0.1165\n",
      "Epoch [56/100], Loss: 0.0514\n",
      "Epoch [57/100], Loss: 0.1694\n",
      "Epoch [58/100], Loss: 0.0331\n",
      "Epoch [59/100], Loss: 0.0751\n",
      "Epoch [60/100], Loss: 0.0372\n",
      "Epoch [61/100], Loss: 0.0962\n",
      "Epoch [62/100], Loss: 0.0125\n",
      "Epoch [63/100], Loss: 0.0888\n",
      "Epoch [64/100], Loss: 0.1212\n",
      "Epoch [65/100], Loss: 0.0191\n",
      "Epoch [66/100], Loss: 0.0759\n",
      "Epoch [67/100], Loss: 0.0537\n",
      "Epoch [68/100], Loss: 0.0597\n",
      "Epoch [69/100], Loss: 0.0353\n",
      "Epoch [70/100], Loss: 0.0448\n",
      "Epoch [71/100], Loss: 0.0565\n",
      "Epoch [72/100], Loss: 0.0255\n",
      "Epoch [73/100], Loss: 0.1846\n",
      "Epoch [74/100], Loss: 0.0467\n",
      "Epoch [75/100], Loss: 0.1606\n",
      "Epoch [76/100], Loss: 0.1132\n",
      "Epoch [77/100], Loss: 0.1804\n",
      "Epoch [78/100], Loss: 0.0270\n",
      "Epoch [79/100], Loss: 0.1163\n",
      "Epoch [80/100], Loss: 0.0280\n",
      "Epoch [81/100], Loss: 0.0283\n",
      "Epoch [82/100], Loss: 0.1326\n",
      "Epoch [83/100], Loss: 0.0566\n",
      "Epoch [84/100], Loss: 0.0129\n",
      "Epoch [85/100], Loss: 0.0397\n",
      "Epoch [86/100], Loss: 0.0167\n",
      "Epoch [87/100], Loss: 0.1011\n",
      "Epoch [88/100], Loss: 0.0285\n",
      "Epoch [89/100], Loss: 0.0250\n",
      "Epoch [90/100], Loss: 0.0890\n",
      "Epoch [91/100], Loss: 0.0486\n",
      "Epoch [92/100], Loss: 0.0332\n",
      "Epoch [93/100], Loss: 0.0784\n",
      "Epoch [94/100], Loss: 0.0082\n",
      "Epoch [95/100], Loss: 0.0424\n",
      "Epoch [96/100], Loss: 0.0785\n",
      "Epoch [97/100], Loss: 0.0116\n",
      "Epoch [98/100], Loss: 0.0456\n",
      "Epoch [99/100], Loss: 0.0764\n",
      "Epoch [100/100], Loss: 0.0466\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()               # clears the gradients before new back prop (new batch)\n",
    "        outputs = model.forward(inputs)     # feed model with forward prop (get predictions - outputs)\n",
    "        loss = criterion(outputs, labels)   # calculate loss value of predictions\n",
    "        loss.backward()                     # perform back prop to compute gradient w.r.t model params\n",
    "        optimizer.step()                    # update the model params (weights) according to LR and gradient\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e36d18",
   "metadata": {},
   "source": [
    "### Evaluation of Model on Test Data\n",
    "Trained NN model is tested on test data which is gathered by splitting the dataset. Accuracy result is obtained as the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cecdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9433\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the model on the test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model.forward(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edef391-72a0-4b69-b188-057f85108aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
